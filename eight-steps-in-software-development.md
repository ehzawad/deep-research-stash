# Eight-Step Software Development Methodology: In-Depth Analysis

**Introduction**  
Developing quality software requires more than just writing code—it demands a disciplined problem-solving approach. The eight-step methodology under study emphasizes understanding the problem deeply, planning a solution with concrete examples, and building software incrementally using scaffolds and placeholders. This approach synthesizes classic problem-solving principles with modern agile practices. For example, mathematician George Pólya’s first rule was **“Understand the problem,”** a step often neglected despite being critical ([How to Solve It - Wikipedia](https://en.wikipedia.org/wiki/How_to_Solve_It#:~:text=,taught%20teachers%20how%20to%20prompt)). Similarly, software engineering pioneer Niklaus Wirth advocated *stepwise refinement*—gradually refining a high-level design into code ([Stepwise refinement](https://www.cs.cornell.edu/courses/JavaAndDS/stepwise/stepwise.html#:~:text=The%20term%20stepwise%20refinement%20,227)). Industry experts echo these ideas: *“A perfect implementation of the wrong specification is worthless,”* warns Tom Preston-Werner (GitHub co-founder) ([Readme Driven Development](https://tom.preston-werner.com/2010/08/23/readme-driven-development#:~:text=I%20hear%20a%20lot%20of,something%20very%20bad%20going%20on)), highlighting the importance of solving the right problem before perfecting code. Experienced engineers tend to invest significant effort upfront in analysis and planning, whereas novices often dive straight into coding and struggle later ([How do you tell an expert from a novice? Look at how they plan.  — WENDY HIRSCH ](https://wendyhirsch.com/blog/planning-best-practice-partii#:~:text=What%20our%20professor%20had%20offered,a%20trial%20and%20error%20approach)). The following sections examine each of the eight steps in detail, providing theoretical background, practical guidance, and examples from **web development**, **game design**, **data analysis**, and **automation** domains. Each step is supported by references to academic research, proven practices in industry, and high-quality technical literature.

## Step 1: Immerse in the Problem  
Before writing a single line of code, an engineer must fully **understand the real-world problem** to be solved. Immersing in the problem means gathering requirements, studying the domain, and even simulating the problem manually. This can involve talking to stakeholders, using physical props or diagrams to model the scenario, and working through the problem step-by-step by hand. The goal is to form a clear mental model of **what** needs to be accomplished *independent of any code*. As Pólya noted, many are *“stymied in their efforts to solve [a problem], simply because they don’t understand it fully”* ([How to Solve It - Wikipedia](https://en.wikipedia.org/wiki/How_to_Solve_It#:~:text=,taught%20teachers%20how%20to%20prompt)). Taking time to clarify objectives and constraints lays a solid foundation for all later steps.

From a theoretical perspective, cognitive studies of expert vs. novice programmers show that experts spend more time analyzing the problem, whereas novices rush into coding ([How do you tell an expert from a novice? Look at how they plan.  — WENDY HIRSCH ](https://wendyhirsch.com/blog/planning-best-practice-partii#:~:text=What%20our%20professor%20had%20offered,a%20trial%20and%20error%20approach)). Experts carefully examine what is being asked, often rephrasing the problem in their own words and identifying core requirements. They focus on high-level goals and patterns, rather than immediately worrying about details. This up-front analysis pays off by reducing false starts. In practice, renowned software engineer John Sonmez advises: **“Resist the urge to start coding immediately. Take enough time to understand the problem completely before attempting to solve it.”** ([Solving Problems, Breaking it Down | by John Sonmez | Medium](https://medium.com/@jsonmez/solving-problems-breaking-it-down-8e7b379edd97#:~:text=The%20most%20common%20mistake%20I,code%20as%20soon%20as%20possible)) Reading the problem description multiple times, identifying unknown terms, and considering expected inputs/outputs are essential sub-steps. It’s often recommended to articulate the problem to someone else (or even to a rubber duck) to ensure you haven’t misunderstood anything.

Understanding the real-world context might involve **physical props or role-playing**. For example, if the task is to optimize the logistics for a delivery service, a developer might use toy trucks and maps to simulate deliveries. Such tangible simulations can spark insights that pure abstract thinking might miss. Sketching is another powerful technique: drawing flowcharts, UI mockups, or state diagrams can make the problem concrete. By manually walking through scenarios, you may discover edge cases or requirements that were not immediately obvious. The mantra *“Nothing can be automated that cannot be done manually”* holds true ([Solving Problems, Breaking it Down | by John Sonmez | Medium](https://medium.com/@jsonmez/solving-problems-breaking-it-down-8e7b379edd97#:~:text=I%20am%20going%20to%20tell,the%20biggest%20secret%20in%20programming))—if you can’t manually work through the problem, you’re not ready to code a solution.

Crucially, immersing in the problem space helps ensure we are solving the **right** problem. In software projects, it’s alarmingly common to implement features that don’t actually meet user needs. This step acts as a safeguard: verifying the goals and success criteria before any implementation. It aligns with the agile idea that the whole team (developers, testers, product owners) should share a clear understanding of *“the needs of those using the software”* ([Readme Driven Development](https://tom.preston-werner.com/2010/08/23/readme-driven-development#:~:text=I%20hear%20a%20lot%20of,something%20very%20bad%20going%20on)). Techniques like user story mapping, requirement workshops, or simply asking “What will success look like?” produce this shared understanding. By the end of Step 1, the developer should be able to explain the problem in plain terms and possibly outline how a person would solve it without a computer.

#### Examples (Step 1: Immerse in the Problem)  
- **Web Development:** A developer tasked with creating an e-commerce checkout system first meets with sales and customers to learn the business process. They sketch the flow of adding items to a cart, entering shipping details, and processing payment on a whiteboard. By role-playing a customer checkout with sticky notes as “products” and “credit cards,” the developer uncovers edge cases (like what if inventory runs out or payment is declined) before any code is written.  
- **Game Design:** Before coding a game, a designer might create a physical board game version of a level. For a platformer game, they use paper cut-outs for the character and obstacles, manually moving the character to simulate jumps and enemy interactions. This immersion reveals whether the game’s rules produce the desired challenge and fun. It might show that a level is too easy or impossible, informing adjustments to game mechanics before implementation.  
- **Data Analysis:** A data scientist tackling a complex analysis (e.g. predicting sales trends) begins by exploring a sample of the dataset in a spreadsheet. They calculate a few sample statistics by hand and draw a rough graph on paper. This manual analysis helps them understand data distributions, outliers, and the exact question being asked. For instance, manually computing the monthly sales average for one product clarifies how the data should be aggregated and what output format is useful.  
- **Automation (Scripting):** When automating a file backup process for a company, an IT engineer first performs the backup **manually**. They take a set of sample files, copy them to a destination, note the time taken, and simulate a recovery from the backup. This exercise reveals requirements like preserving file permissions or the need for logging errors. Only after doing it by hand (and fully grasping each step) does the engineer start thinking about writing a script.

## Step 2: Design the Ideal Solution  
Once the problem is well-understood, the next step is to **design an ideal solution by hand, step-by-step**. This means figuring out how *you* would solve the problem manually or conceptually, then formalizing those steps. Essentially, you’re creating a blueprint or algorithm in plain language. A key aspect is using **concrete examples and test cases** to drive the design. By walking through specific scenarios with actual sample data, you can define what the correct outcome should be for each step of the process. This example-driven design is akin to the practice of *specification by example*, where concrete cases clarify abstract requirements.

Theoretical foundations for this step come from stepwise problem-solving techniques. George Pólya’s second principle was *“Devise a plan”* ([How to Solve It - Wikipedia](https://en.wikipedia.org/wiki/How_to_Solve_It#:~:text=Second%20principle%3A%20Devise%20a%20plan))—enumerate possible strategies and pick one that seems most promising. In software, this often translates to writing out the solution in pseudocode or structured English. Importantly, this plan should be **validated with examples**. As John Sonmez emphasizes, *“solve the problem manually with 3 sets of sample data”* to ensure your approach works broadly ([Solving Problems, Breaking it Down | by John Sonmez | Medium](https://medium.com/@jsonmez/solving-problems-breaking-it-down-8e7b379edd97#:~:text=I%20recommend%20that%20you%20solve,for%20more%20than%20one%20case)). By trying a simple case, a slightly more complex case, and an edge case, you get confidence that the steps you propose can handle various inputs. During this process, you might optimize the manual steps—eliminating unnecessary work or generalizing a procedure that worked for one example to apply to others ([Solving Problems, Breaking it Down | by John Sonmez | Medium](https://medium.com/@jsonmez/solving-problems-breaking-it-down-8e7b379edd97#:~:text=)).

**Laying out the solution steps in detail** forces you to confront the logic without the aid (or distraction) of coding. You essentially act as a human computer: given input X, what exact operations would you perform to produce output Y? This often surfaces hidden complexities. For instance, designing an “ideal” solution for a scheduling app might reveal that you need to consider calendar conflicts or time zone conversions—things that weren’t obvious until you simulated a scheduling scenario in detail. It’s much better to discover these in the design phase than halfway through coding. As one developer aphorism goes: *“If you can’t describe what the code should do in plain language, you aren’t ready to implement it.”* This step is where that description is created.

Practically, many developers write down these solution steps as comments or pseudocode first. This is reflected in methodologies like **Pseudocode Programming Process (PPP)** from *Code Complete* ([Pseudocode or Code?](https://blog.codinghorror.com/pseudocode-or-code/#:~:text=One%20of%20those%20chapters%20describes,we%E2%80%99d%20first%20write%20it%20in%C2%A0pseudocode)) ([Pseudocode or Code?](https://blog.codinghorror.com/pseudocode-or-code/#:~:text=Then%2C%20when%20you%E2%80%99re%20satisfied%20that,code%20you%E2%80%99re%20about%20to%20write)). The idea is to describe what each part of the program *should* do in natural language (or simple structured steps) *before* worrying about syntax. For example, if designing a sorting algorithm, your ideal solution might be written as: “1. Repeat until no unsorted items remain: find the smallest remaining item and add it to the result.” Writing such steps using a concrete example list (say [8, 3, 1]) helps ensure you haven’t missed anything — you would trace: smallest of [8,3,1] is 1, move it, etc., to see the algorithm in action. Notably, this approach of “write it in English first” has two benefits: (1) It separates *what* the code should accomplish from *how* to implement it, reducing conceptual errors ([handout-08](https://www.cs.cornell.edu/courses/cs1110/2017fa/lectures/09-14-17/handout-08.pdf#:~:text=%E2%80%A2%20Good%20programmers%20know%20how,specification%20before%20writing%20its%20body)); (2) It makes review easier, since peers can understand the logic without diving into code syntax ([Pseudocode or Code?](https://blog.codinghorror.com/pseudocode-or-code/#:~:text=,of%20them%20becomes%20a%20problem)).

Designing the ideal solution also means defining expected outputs for given inputs — effectively creating an **acceptance test** for your solution logic. For instance, if the problem is to format a report, you might manually craft exactly how a correct report looks for a sample dataset. This manual “output” serves as a goalpost to verify your code later (we’ll see this in Step 5). In test-driven development (TDD) terms, you are specifying the behavior before implementation. As the Jane Street developers note: you *“want to define up front the properties you care about, [and] the output you’re expecting”* ([Jane Street Tech Blog - What if writing tests was a joyful experience? ](https://blog.janestreet.com/the-joy-of-expect-tests/#:~:text=I%20hear%20you%20already%3A%20tests,to%20write%20tests%20for%20it)), especially in a test-first approach. That ensures you know what success looks like.

By the end of Step 2, you should have a clear, stepwise solution outline (the algorithm or process) that could, in theory, be followed by a person to solve the problem. You have also worked through representative examples, so you have confidence in the solution’s correctness. Moreover, you’ve identified test cases and their expected outcomes, forming a basis to check your work later. This idealized solution is like a **roadmap** for the implementation: it doesn’t have to consider coding minutiae yet, but it should be logically sound and handle the important cases.

#### Examples (Step 2: Design the Ideal Solution)  
- **Web Development:** Suppose you’re building a password reset feature for a website. After understanding the requirements, you design the solution flow using a concrete example: *User Alice has forgotten her password*. You outline steps: “(1) Alice enters her email on the reset form; (2) System generates a unique token and email it to Alice; (3) Alice clicks the link with the token; (4) System verifies token and prompts for a new password; (5) Alice enters new password, which is validated and saved.” Using a test scenario (Alice’s email is `alice@example.com`), you even simulate what the email looks like and ensure the steps cover cases like invalid or expired tokens. This step-by-step solution in plain language and examples will guide the code.  
- **Game Design:** You’re developing a simple **AI for enemy behavior** in a game. You take an example: the player’s character is at position (5,5) and an enemy is at (0,0) in a grid. Manually, you plan the ideal enemy behavior: “The enemy should move toward the player. Step 1: compare x-coordinates (0 vs 5) and y-coordinates (0 vs 5). Step 2: increment the smaller coordinate to move diagonally towards (5,5).” Walking through this example, you get a sequence of moves: (0,0) -> (1,1) -> (2,2) ... -> (5,5). You also consider edge cases like if an obstacle is in the path (maybe then the ideal solution would be to go around it). By designing and tracing this logic with example positions, you’ve formulated an algorithm to implement.  
- **Data Analysis:** You need to write a program that identifies trends in sales data. After immersing in the sales domain, you take a small sample dataset (e.g., sales of 5 products over 6 months) and **manually compute** the trend for one product: say product A’s sales [100, 120, 90, 150, 130, 170] over Jan–Jun. You decide an ideal solution is: “Calculate month-to-month changes and see if there’s a consistent increase or decrease.” By hand, you compute differences ([+20, -30, +60, -20, +40]) and observe it’s mostly increases. So your designed solution might be: (1) for each product, compute monthly changes; (2) count how many increases vs decreases; (3) if >70% are increases, label trend “upward”, etc. You test this logic on a couple of manual examples (including one with all decreases to label “downward”). These example-driven rules form your plan for coding.  
- **Automation:** You are tasked with automating employee onboarding (creating accounts, sending welcome emails, etc.). Having understood the manual HR process, you outline the ideal automated workflow with a specific example: *Onboard employee Bob on his start date*. Steps: “(1) Read Bob’s info from HR system; (2) Create email account for Bob; (3) Add Bob to relevant mailing lists; (4) Generate an email with credentials and send to Bob; (5) Schedule a reminder to IT for equipment.” By writing these steps in order and considering Bob’s example data (department, role, etc.), you refine the details (e.g., which mailing lists?). You also imagine a test case where some data is missing (what if Bob’s department isn’t specified? Perhaps step 3 would be skipped or use a default). This gives you a clear algorithm to implement in a script, covering the normal path and a few edge conditions, all derived from concrete onboarding scenarios.

## Step 3: Draft a Code Structure Plan  
With a clear algorithm in mind, the next step is to **plan the code structure** that will implement the solution. This involves defining the modules, functions, loops, and conditional structures that your program will need. Essentially, you’re translating the step-by-step “ideal solution” from Step 2 into a skeletal program design. This plan is often captured as pseudocode, flowcharts, or high-level descriptions of components and their responsibilities. The key is to map *each part of your solution logic to a part of the program*. By the end of this step, you should have a blueprint of how the code will be organized — for example, which functions will handle which subtasks, what data structures will be used, and how those functions will sequence together.

In software engineering theory, this corresponds to **top-down design** or **stepwise refinement**. You start with the broad tasks and break them into subtasks, mirroring Wirth’s approach of decomposing “tasks into subtasks and data into data structures” ([Stepwise refinement](https://www.cs.cornell.edu/courses/JavaAndDS/stepwise/stepwise.html#:~:text=Wirth%20said%2C%20,of%20data%20into%20data%20structures)). Each subtask might become a function or a method in code. Edsger Dijkstra and Niklaus Wirth promoted this method to manage complexity: tackle the problem at the highest level first, then gradually refine details ([Stepwise refinement](https://www.cs.cornell.edu/courses/JavaAndDS/stepwise/stepwise.html#:~:text=The%20term%20stepwise%20refinement%20,227)). Practically, this might mean writing a function stub (a placeholder function) for each sub-problem identified in your ideal solution. For instance, if the problem is solving a maze, you might plan to have functions like `findPath()` and `isDeadEnd()`, even if you haven’t implemented them yet. This *modular thinking* ensures your code plan covers all aspects of the solution.

A **code structure plan** often takes the form of pseudocode outline. For example, continuing the password reset example from Step 2, you might sketch something like:

```
function resetPassword(email):
    token = generateToken(email)
    sendEmail(email, token)
    return "Reset link sent"

function handleResetLink(token, newPassword):
    if not isValid(token): return "Invalid or expired link"
    user = getUserByToken(token)
    user.password = hash(newPassword)
    return "Password updated"
```

This is a rough plan where each function is a piece of the puzzle (with some details abstracted out). Notice it’s not full code – it’s a scaffold indicating structure. The goal of this step is not to solve everything at once, but to lay out *where everything will go*. This aligns with McConnell’s Pseudocode Programming Process, which advocates writing pseudocode for each routine and refining it gradually ([Pseudocode or Code?](https://blog.codinghorror.com/pseudocode-or-code/#:~:text=One%20of%20those%20chapters%20describes,we%E2%80%99d%20first%20write%20it%20in%C2%A0pseudocode)) ([Pseudocode or Code?](https://blog.codinghorror.com/pseudocode-or-code/#:~:text=Then%2C%20when%20you%E2%80%99re%20satisfied%20that,code%20you%E2%80%99re%20about%20to%20write)). By writing such structured pseudocode/comments first, you effectively review your design in logical terms. You can catch mistakes now (maybe you forgot what happens if the email doesn’t exist, etc.) rather than later in the middle of coding ([Pseudocode or Code?](https://blog.codinghorror.com/pseudocode-or-code/#:~:text=,of%20them%20becomes%20a%20problem)).

Another benefit of a code plan is **clarity for others (and your future self)**. It serves as a design document. Teams often perform design reviews at this stage, walking through the proposed functions and logic without yet having low-level code. It’s much easier to adjust the plan at this point (e.g., split a function into two, change a data flow) than to refactor working code after the fact. By keeping the plan at a high level, changes are “cheap” — *“a few lines of pseudocode are easier to change than a page of code”* ([Pseudocode or Code?](https://blog.codinghorror.com/pseudocode-or-code/#:~:text=and%20low,on%20a%20blueprint%20or%20rip)).

It’s worth noting that modern agile methodologies don’t preclude upfront design. Techniques like **UML diagrams** or informal whiteboard sketches of class structure are common. The difference is that this code structure plan is lightweight and *modifiable*, not a frozen spec. We expect to refine it, but it provides a guiding architecture. Martin Fowler describes how even in evolutionary design, having a minimal **architecture sketch** (like a “walking skeleton,” which we’ll discuss later) is crucial to give the team an end-to-end picture ([lifelong-learning/programming/ruby/testing/growing-object-oriented-software-guided-by-tests.md at master · daryllxd/lifelong-learning · GitHub](https://github.com/daryllxd/lifelong-learning/blob/master/programming/ruby/testing/growing-object-oriented-software-guided-by-tests.md#:~:text=Deciding%20the%20Shape%20of%20the,Walking%20Skeleton)).

In summary, Step 3 is about **bridging the gap between abstract solution and actual code**. You decide how to partition the logic into functions or modules, how data will flow, and what major loops or conditionals are needed. The output is typically a set of function prototypes or pseudocode that covers the whole task at a high level. By following this plan, you can implement systematically in the next steps, knowing that each part fits into an overall structure.

#### Examples (Step 3: Draft a Code Structure Plan)  
- **Web Development:** After designing the ideal flow for an e-commerce checkout, a developer drafts the code structure. They outline an **MVC (Model-View-Controller)** structure: *Model* classes for `Order` and `Payment`, a *Controller* method `checkout(orderId)` that orchestrates the process (validate cart, charge card, create order record), and *View* templates for the cart page and confirmation page. In pseudocode, the controller might be written with placeholders: “`if payment.success then Order.save else showError`”. They also list helper functions (e.g., `calculateTax()`, `reserveInventory()`) as stubs. This modular plan makes it clear what needs to be implemented and avoids forgetting any step of the checkout logic.  
- **Game Design:** A game developer planning the code for a platformer level decides on a structured game loop and module separation. They sketch a flowchart: each frame, call `updatePhysics()`, then `detectCollisions()`, then `renderFrame()`. They outline classes like `Player`, `Enemy`, `LevelMap`, each with certain methods (`Player.move()`, `Enemy.patrol()` etc.). For example, pseudocode for the main loop:  
  ```  
  while gameRunning:  
      for entity in allEntities:  
          entity.update()    # moves characters, handles AI  
      handleCollisions(allEntities)  
      render(allEntities)  
  ```  
  By drafting this, the developer clarifies how components interact. They also decide to stub out `handleCollisions` as a function they’ll implement later. The plan ensures the game’s structure is sound (each frame updates and renders consistently) before filling in details.  
- **Data Analysis:** A data engineer planning a data pipeline writes out a structure in code-like terms. For a program that reads raw data, filters it, then computes statistics, the engineer drafts a pipeline:  
  ```  
  def main():  
      data = load_data("input.csv")  
      clean_data = filter_invalid(data)  
      results = compute_statistics(clean_data)  
      save_results(results, "output.csv")  
  ```  
  Underneath, they note that `filter_invalid` will remove entries with missing fields, `compute_statistics` will perhaps call sub-functions like `compute_average` and `compute_trend`. This top-level scaffold shows the sequence of operations. They may also decide on data structures (e.g., use a list of dicts for records, or a DataFrame). By laying this out, they have a clear plan of attack for coding, and they can review it to ensure no step is overlooked (e.g., do they need a `sort_data` step? If so, they’d insert it now).  
- **Automation:** For an automation script that monitors servers and sends alerts, an IT developer plans the script structure. They break it into components: one function to fetch server metrics, another to analyze those metrics, and another to send an alert email. The draft might look like:  
  ```  
  metrics = get_metrics(server_list)  
  report = analyze(metrics)  
  if report.has_issue:  
      send_alert(report)  
  schedule_next_run(delay=5min)  
  ```  
  Each of those tasks (`get_metrics`, `analyze`, `send_alert`) is identified as a separate function (or even separate scripts or classes). By drafting this pseudo-code, the developer also plans for looping (scheduling every 5 minutes) and error handling (what if a server is unreachable? – maybe note to handle exceptions inside `get_metrics`). This structured outline guides implementation and ensures the script will cover data collection, analysis, and notification in the correct order.

## Step 4: Perform Self-Assessment of Implementation Knowledge  
After drafting the code structure, a crucial reflective step is to **assess your own understanding and identify any knowledge gaps**. In Step 4, the developer evaluates each part of the plan and asks: *“Do I know how to implement this?”* For the parts where the answer is “yes,” great—one can proceed with confidence. For the parts that are uncertain or unknown, this step calls for research, prototyping, or seeking advice *before* diving into full implementation. It’s essentially a risk mitigation practice: by pinpointing which pieces of the plan you’re less sure about, you can address those uncertainties upfront (through experiments or learning) rather than getting stuck later.

In terms of best practices, this step aligns with the concept of **“spikes”** in agile development. A spike is a time-boxed investigation or prototype to answer a question or resolve ambiguity in a user story ([What is an Agile Spike Story? How to Write, Types & Benefits](https://agilemania.com/agile-spike-story-what-is-a-spike-in-agile#:~:text=What%20is%20an%20Agile%20Spike)). Extreme Programming (XP) introduced spikes specifically to handle aspects of the plan that the team does not fully understand (be it a new technology, a tricky algorithm, or integration with an external system). For instance, if your plan requires using a third-party API you’ve never worked with, you might do a quick spike to call a couple of its endpoints in a sandbox environment. The Scaled Agile Framework defines spikes as experiments to *“gather information to reduce risk and enable more accurate estimates for complex user stories.”* ([What is an Agile Spike Story? How to Use, Types & Benefits](https://agilemania.com/agile-spike-story-what-is-a-spike-in-agile#:~:text=What%20is%20an%20Agile%20Spike,estimates%20for%20complex%20user%20stories)). In the personal context of a developer following our eight-step methodology, the “user story” is your feature/problem, and the spike corresponds to you learning or prototyping the unknown pieces.

A theoretical underpinning for this step is the Dunning-Kruger effect or simply the practice of **metacognition** in programming – being aware of what you know and don’t know. Successful developers make it a habit to pause and reflect on their plan. If, for example, your plan in Step 3 includes “Apply Dijkstra’s algorithm to find shortest path,” Step 4 is when you ask: Have I implemented Dijkstra’s before? Do I remember it well enough? If not, you identify that as a gap and perhaps read up an algorithm reference or find a library that implements it. This up-front honesty can save hours of frustration. As the old saying goes, *“Plan to throw one away; you will, anyhow”* (Fred Brooks) – which hints that your first attempt might be a learning exercise. Step 4 encourages you to consciously learn what’s needed so the “first attempt” is informed.

In practice, this self-assessment can be done by marking each component in your code structure with a confidence level. For example: `function sendEmail` – confident (done similar before); `function generateToken` – uncertain (need to research secure random token generation); `function hashPassword` – know the concept but might need a library (note to check Python’s hashlib library). By annotating these, you create a mini to-do list for learning. You then tackle those learnings: read documentation, search online, ask colleagues, or build a small prototype. For instance, if you need to use OAuth for the first time, you might do a quick test integration with a demo account outside of your main project. This is analogous to building a *proof-of-concept* for that piece ([What is an Agile Spike Story? How to Write, Types & Benefits](https://agilemania.com/agile-spike-story-what-is-a-spike-in-agile#:~:text=Sometimes%2C%20teams%20struggle%20to%20estimate,analyze%20potential%20solutions%20and%20unknowns)).

Academic research in software engineering also stresses the importance of developer knowledge on project outcomes. Studies on productivity show that time spent in **feasibility analysis** or technology evaluation is a good investment when dealing with unfamiliar areas, as it prevents costly rewrites later. By doing Step 4, developers also refine their estimates: if initially a task looked simple but on reflection you realize you need to learn a lot, you can adjust the plan or timeline accordingly, rather than being surprised mid-implementation.

Another outcome of this step may be deciding to **simplify the plan** if an element is too complex. For example, if you realize you don’t know how to implement an optimal solution due to knowledge gap, you might choose a simpler suboptimal method first (and maybe note to improve it later). This ties into agile’s iterative nature: implement what you’re capable of, and improve as you learn.

By the end of Step 4, you should have a clear idea of which parts of your project are straightforward and which require additional preparation. You may have done several small “spikes” or research tasks to equip yourself with the necessary knowledge. The result is reduced risk going forward — you won’t hit a complete roadblock in the middle of development, because you proactively surfaced and addressed the toughest questions. In effect, you’ve **de-risked** your implementation plan.

#### Examples (Step 4: Self-Assessment and Knowledge Acquisition)  
- **Web Development:** A web developer has planned to implement OAuth 2.0 login in a new app. In self-assessment, they realize they’ve never worked with OAuth flows. To address this, they do a mini-project: they follow an online tutorial to set up a Google OAuth login on a dummy app, just to see how the redirect and token exchange works. This spike teaches them about redirect URIs and scopes. With that knowledge, they return to the project with confidence on the OAuth part. Additionally, they identify another unknown: sending emails from the server. They quickly research an email-sending library (like NodeMailer or SMTP integration) and perhaps test it in isolation. By filling these knowledge gaps, the developer prevents delays later on.  
- **Game Design:** A game programmer’s plan for a 3D game includes a physics engine for realistic object collisions. On self-review, the programmer notes they are unsure how to use the chosen physics library (say Unity’s physics or Box2D). They decide to prototype a small scene with just two objects colliding and bouncing, to familiarize themselves with the API (gravity, friction settings, collision callbacks). This experiment reveals how to configure colliders properly. They also were planning an advanced pathfinding algorithm (A*). If they’ve never implemented A* before, at this point they might watch a tutorial or implement A* on a simple grid to grasp it. Alternatively, they might decide to use an existing pathfinding library if learning A* from scratch would take too long.  
- **Data Analysis:** A data analyst’s program plan involves using a machine learning model (say a random forest) to predict outcomes. During Step 4, the analyst checks their understanding: they know how to train a model in Python with scikit-learn, but deploying the model for live predictions is new to them. Recognizing this, they research how to serialize models (using joblib) and test loading a model from disk in a small script. They also realize they’re not fully sure how to evaluate the model’s accuracy properly. So they read a few academic papers or documentation on evaluation metrics (e.g., precision, recall) and decide which metric suits the problem. With this knowledge, they feel ready to implement the model training and evaluation steps in the main project.  
- **Automation:** An IT automation engineer plans a script to interact with a cloud provider’s API to spin up servers. On reviewing the plan, they find two areas of uncertainty: (1) the API authentication mechanism (maybe it uses OAuth tokens or API keys); (2) parsing JSON responses in their chosen scripting language (PowerShell or Bash). They address these by reading the cloud provider’s API docs and writing a short test script that authenticates and fetches a simple piece of data (like listing current servers). This hands-on trial uncovers an gotcha (perhaps the API requires a specific header or has a rate limit). Forewarned, the engineer can incorporate those findings. They also quickly brush up on JSON parsing by experimenting with a small JSON snippet. Through these preparatory exercises, the engineer ensures they have the know-how to implement the full automation without hitting unexpected snags.

## Step 5: Build the Final Output by Hand (Acceptance Test Preparation)  
Step 5 is about creating a **reference output** or outcome by hand, which will serve as a benchmark for your program. In other words, you manually produce what you expect the program to produce, in a polished and correct form. This could be an example of a final report, a screen mock-up with the expected results, or any artifact that represents success. The idea is to have a **concrete acceptance test** before fully coding – a trustworthy point of comparison to verify the program’s correctness when it’s done.

In test-driven development (TDD) and behavior-driven development (BDD), a similar concept exists: you define expected outputs (tests or scenarios) upfront. Here, “build the final output by hand” means doing that even outside of code. For example, if writing a function that transforms data, you would take a sample input dataset and manually compute the correct transformed result. That manual result is essentially the expected output you will later compare against the program’s output. As an analogy, think of a math problem: you first solve it on paper (perhaps laboriously) to know the correct answer, then you write code and ensure the code’s answer matches the one on paper.

Why is this important? One reason is to validate that your understanding of the problem and solution is correct. If you cannot manually produce a final result for a test case, perhaps the problem is still not fully clear. Another reason is to have an **oracle** for testing – in software testing, an oracle is a mechanism to determine if the output is correct. Your hand-crafted output serves as an oracle. Researchers and practitioners have long noted that having expected results greatly simplifies debugging: if your code’s output differs from the hand-produced output, you know something is wrong. It essentially closes the loop on requirements – ensuring the program does what you (and stakeholders) expect.

From a theoretical standpoint, this step resonates with **acceptance test-driven development (ATDD)** and **Specification by Example** (as popularized by Gojko Adzic). In ATDD, the team writes an acceptance test in plain language (often given-when-then scenarios) before implementation. For instance, an acceptance criterion for a feature might be: *Given* a user with an expired password, *when* they try to log in, *then* they are prompted to reset their password. Here, building final output by hand could be writing the exact message that should be shown. Adzic’s approach would have the team document an example (input and expected output) in a table or narrative form. These serve as the source of truth for correct behavior ([Best Practices - Specification by Example - Weekly Sharing - ZenTao](https://www.zentao.pm/blog/best-practice-specification-by-example-1597.html#:~:text=ZenTao%20www.zentao.pm%20%20,requirements%20through%20practical%20examples%2C)). Our methodology’s Step 5 is very much in line with this: it says don’t just *imagine* the correct output, actually **create it manually** so it’s tangible and agreed upon.

In practice, this might involve creating artifacts like: a sample **output file**, a screenshot mockup, or a log of expected operations. For a UI feature, one might design the final screen in a design tool or sketch it, showing exactly what data appears. For a report, one might use Excel to manually compute the results and format them nicely, then save that as the “golden sample.” For an algorithmic problem, one might run a small instance of the problem by hand – e.g., manually sort a list, manually step through an encryption by hand for a short message – to have the known correct result. This manually obtained result is often called a **“golden result”** or **“baseline.”** In some testing frameworks, they use “golden files” where the expected output is stored and later the program’s output is diffed against it ([Testing full program by comparing output file to reference file](https://stackoverflow.com/questions/9726214/testing-full-program-by-comparing-output-file-to-reference-file-whats-it-calle#:~:text=Testing%20full%20program%20by%20comparing,the%20program%20on%20the)).

One benefit of doing this step is psychological as well: it shifts your mindset to the *user’s perspective* or the *output perspective*. Instead of thinking about the internals of code, you focus on what the end product should look like. This can highlight if your earlier steps missed something. For example, while composing a final report by hand, you might realize a certain summary section is needed that you hadn’t accounted for; you can then incorporate that into your plan *before* coding. It enforces a check: is this output actually addressing the initial problem? If not, better to refine now.

Moreover, having the final output example makes verification straightforward. As noted in a tech blog on testing: *“You want to define up front ... the output you’re expecting, (especially in TDD). You don’t want to wait for your function to be written to decide what the output should be.”* ([Jane Street Tech Blog - What if writing tests was a joyful experience? ](https://blog.janestreet.com/the-joy-of-expect-tests/#:~:text=I%20hear%20you%20already%3A%20tests,to%20write%20tests%20for%20it)). By following that advice, when the code is eventually written, a simple comparison with the hand-built output can confirm success or failure. If the outputs match, you’ve essentially passed an acceptance test; if not, further debugging is guided by the differences.

In summary, Step 5 produces a **manual benchmark** for correctness. It solidifies the target you’re aiming for. This step often separates high-quality, testable development from ad-hoc coding—by demanding a clear definition of “done” (i.e., what the correct output looks like) *before* implementing, it reduces ambiguity. When you later run your program on the same example and it produces identical output, you gain high confidence that the program works as intended.

#### Examples (Step 5: Build Final Output by Hand)  
- **Web Development:** Imagine you are building a complex **dashboard page** that shows analytics charts and tables. Before coding, you gather sample data and manually create the dashboard in a spreadsheet or design tool. For example, you might use Excel to generate a chart of user sign-ups over time, a table of the top 10 users, etc., using dummy data that resembles production. You polish this “report” until it looks exactly how the client wants (colors, format, calculations all correct). This static mockup (or an HTML file crafted by hand) is the final output by hand. Once the code is written, the developer can compare the web application’s generated dashboard against this reference to ensure every number and element matches the agreed expectation.  
- **Game Design:** A game developer is working on a **level completion summary screen** (that shows stats like score, enemies defeated, time taken). In this step, they simulate playing through a level and record all the stats manually: e.g., “Score: 15,340; Enemies defeated: 10; Time: 02:15”. Then they sketch or design the exact layout of the summary screen with those stats. This might involve using an image editor to place the numbers on a background. This mockup represents the ideal final screen for that test run. Later, when implementing the screen in the game, they will run the level under similar conditions and verify that the summary screen matches the mockup (15,340 points, etc.). If something is off (say the time format is different), they know there’s a discrepancy to fix.  
- **Data Analysis:** Suppose you are writing a data analysis script that outputs a **CSV report** of sales trends. In Step 5, you take a small sample input (perhaps January sales for 5 products) and manually calculate everything the report should contain: total sales per product, percent change month-over-month, rankings, etc. You put these results into a CSV file or spreadsheet by hand, double-checking calculations. This hand-made CSV is your golden reference. Once you implement the analysis in code, you run the script on the same sample input and compare its output CSV to the hand-made one. Any differences indicate a bug. Because the manual output was prepared carefully, you trust it as the standard for correctness.  
- **Automation:** An example in automation could be an **automated email generation**. Say you’re automating a weekly summary email that goes out to a team. In this step, you manually write a sample email: “Subject: Weekly Report – Oct 1–7, Body: ...” including all the content that should be there (perhaps you pull last week’s data and actually craft the email as if the script had done it). You ensure the formatting is nice and all pieces of information are present. This serves as the expected email output. When your automation script is eventually run (perhaps with the same week’s data), you compare the actual email it sends against the manually written one. If they match (minus any trivial differences), you know the automation worked correctly. If the script’s email is missing a section or has wrong figures, you catch it by comparing to the hand-built reference.

## Step 6: Create a Running Scaffold  
With the problem understood, solution designed, code structure planned, knowledge gaps addressed, and expected output known, it’s time to start coding—*but not the final code yet*. Step 6 is about creating a **running scaffold** for your program: a skeletal version of the program that can execute end-to-end, albeit with many parts empty or simplified. The idea is to write the top-level flow of the program (the “plumbing” or wiring between components) with placeholder functions or modules (“stubs”) for the pieces that will be filled in later. This results in a program that **does nothing useful yet, but *does run*** without crashing. It’s often called a *walking skeleton* in agile terminology: the thinnest possible slice of a working system that covers the main architecture ([lifelong-learning/programming/ruby/testing/growing-object-oriented-software-guided-by-tests.md at master · daryllxd/lifelong-learning · GitHub](https://github.com/daryllxd/lifelong-learning/blob/master/programming/ruby/testing/growing-object-oriented-software-guided-by-tests.md#:~:text=To%20solve%20this%2C%20we%20split,for%20the%20first%20meaningful%20feature)) ([Walking Skeleton in Software Architecture | Medium](https://medium.com/@jorisvdaalsvoort/walking-skeletons-in-software-architecture-894168276e3f#:~:text=Martin%20Fowler%20described%20the%20walking,skeleton%20as)).

The theoretical basis of this step comes from **top-down implementation** and the *mañana principle*. The mañana (tomorrow) principle in programming suggests *“If something isn’t needed now, postpone its implementation to later (mañana)”*, replacing it with a stub or placeholder ([handout-08](https://www.cs.cornell.edu/courses/cs1110/2017fa/lectures/09-14-17/handout-08.pdf#:~:text=Ma%C3%B1ana%20Principle%20%E2%80%A2%20If%20not,function%20definition%20empty%20for%20now)). By doing so, you can compile and run the program frequently from the start, catching integration issues early. As an academic source notes, *“Add ‘stubs’ to allow you to run [the] program often… Slowly replace stubs/comments with real code”* ([handout-08](https://www.cs.cornell.edu/courses/cs1110/2017fa/lectures/09-14-17/handout-08.pdf#:~:text=Ma%C3%B1ana%20Principle%20%E2%80%A2%20If%20not,function%20definition%20empty%20for%20now)). This approach was advocated in stepwise refinement and is taught in introductory programming as a way to manage complexity. The scaffold usually consists of a main routine that calls subroutines that are not yet implemented (they might just return a dummy value or print a message like “TODO”). Crucially, the scaffold sets up the program’s **structure in code** and ensures that all the pieces connect properly. This addresses integration risks early — it verifies that modules interface correctly (function signatures, data passing) even before their internals are completed.

A **walking skeleton** in agile architecture is exactly this idea at the system level: Alistair Cockburn described it as *“a tiny implementation of the system that performs a small end-to-end function… linking together the main architectural components”* ([Kickstart Your Next Project with a Walking Skeleton | Code Climate](https://codeclimate.com/blog/kickstart-your-next-project-with-a-walking-skeleton#:~:text=end,1)). It might not deliver real features, but it touches all major parts (e.g., database, API, UI) in a minimal way. The benefit is that you discover any major architectural incompatibilities or deployment issues at the start, not at the end. Martin Fowler also encourages this, noting it prevents investing too much in an architecture based on assumptions that haven’t been validated by a running system ([Walking Skeleton in Software Architecture | Medium](https://medium.com/@jorisvdaalsvoort/walking-skeletons-in-software-architecture-894168276e3f#:~:text=It%20is%20used%20to%20ensure,backend%2C%20database%2C%20you%20name%20it%E2%80%A6)).

In practice, creating a running scaffold means writing code like: a `main()` function that calls all the major steps (which currently do little). For example, you might write `main()` to call `load_data()`, `process_data()`, `output_results()`, even if those functions currently just contain `pass` or return dummy values. If using an object-oriented approach, you might create class definitions with empty method bodies just so you can instantiate objects and call methods in the right order. The program might output hardcoded placeholders or very simple static output at this stage, just to satisfy the flow.

It’s important that the scaffold *runs without errors*. That means hooking up any required environment or framework pieces (maybe an empty database for now, or minimal configuration) so that you can execute the program and get a trivial result (like a program that immediately exits after exercising the skeleton). As a best practice, “your program with stubs should still run” and every stubbed function should be invoked somewhere in the program ([](https://www.cs.swarthmore.edu/~alinen/cs21/f18/w07/Week07-TDD.pdf#:~:text=Code%20stubs%20,you%20expect%20it%20to%20use)). This ensures no part of the planned structure is forgotten and that each part is reachable.

One might question: why run something that barely does anything? The advantage is it tests your assumptions about integration. For instance, you might find that two modules have a mismatch in how they communicate (maybe one returns data in a format the other doesn’t expect). By stubbing them and running, you catch that early. Additionally, having a scaffold allows continuous testing as you fill in functionality (Step 7 and 8): you always have a runnable system, which is the essence of continuous integration. It also gives you a psychological boost — you *see* something running, which can be motivating, even if it’s just a skeleton.

Another benefit of scaffolding is that you can start writing **basic tests** against the scaffold. For example, you could have a unit test that calls an API endpoint which currently returns a dummy response; the test can at least hit the endpoint and get a response (even if it’s “Not implemented”). This ensures your test harness and deployment pipeline are in place early on. In fact, literature on incremental development often recommends getting a trivial end-to-end test passing as soon as possible ([Kickstart Your Next Project with a Walking Skeleton | Code Climate](https://codeclimate.com/blog/kickstart-your-next-project-with-a-walking-skeleton#:~:text=In%20order%20to%20reduce%20risks,1)).

In summary, Step 6 produces a **thin, skeletal version of the program** that establishes the structure and is capable of running end-to-end with fake implementations. It’s like constructing the scaffolding of a building: the shape is there, but the walls and details aren’t built yet. This sets the stage for the next steps, where actual functionality will be gradually added onto this scaffold.

#### Examples (Step 6: Create a Running Scaffold)  
- **Web Development:** Suppose the project is a web application with a front-end, back-end, and database. The team creates a scaffold by setting up the basic project structure: perhaps a simple homepage route on the server that queries a database table. At first, the database table might be almost empty or filled with dummy data, and the query code just selects a constant. The homepage template might just display “Hello, World” and a placeholder for user info. They deploy this minimal app to a dev server. It doesn’t do anything useful, but it confirms that the web server is correctly configured, it can connect to the (empty) database, and the end-to-end request cycle works. All future development will be done on this running baseline. For instance, a developer might stub out routes for “/login” and “/dashboard” that currently return “Not implemented” pages, just to ensure the URLs are wired in.  
- **Game Design:** For a game, a scaffold could mean getting the game loop running with a simple scene. The developer might create an empty game level where a single player object can move around, but there are no enemies, no scoring, etc. The player controls might be implemented (e.g., arrow keys move the character), but if the character collides with where an enemy should be, nothing happens because the enemy logic is a stub. The game doesn’t fulfill the gameplay yet, but it **runs**: the window opens, the background renders, the character moves. Perhaps the scaffold includes an “updateEntities()” call that iterates through an enemy list, which is empty for now. By doing this, the developer ensures the game’s core loop, rendering pipeline, and input system are all hooked up. It’s a walking skeleton of the game – the structure of a playable level without the actual challenges.  
- **Data Analysis:** A data pipeline scaffold might involve setting up the main script to go through the motions on a tiny dummy dataset. For instance, the engineer writes the `main()` to call `load_data -> process_data -> save_results` as planned. They implement `load_data` to just load a very small static CSV (maybe 2 lines of fake data), `process_data` might currently just compute a trivial value (or even do nothing and just pass the data through), and `save_results` writes out something like “Pipeline ran successfully” to an output file. When they run this scaffold pipeline, it reads the fake data, “processes” it (no-op), and outputs a confirmation. This test run validates file paths, library imports, and overall script execution. It’s also integrated with whatever infrastructure needed (perhaps the script is scheduled via cron — they test that the scheduling triggers it properly using this scaffold).  
- **Automation:** Consider an automation scenario where multiple components have to work together, say a script that monitors a server and then sends an SMS alert via a third-party service. A scaffold here might involve writing the script with the full structure: it has a function `check_server()` and a function `send_sms()`, and a `main` that calls them. Initially, `check_server()` doesn’t actually check anything important – it might just return a fixed “all good” status. `send_sms()` could be stubbed to just print “SMS sent to [number]” to console instead of actually integrating with the service. The script then ties these together: if status not good, call send_sms. The developer runs the script (perhaps scheduling it to run every minute as intended). It will always report all good and not really send texts, but the flow is executed. This flushes out, for example, that the SMS API library was imported correctly (or if credentials are needed, they might realize it now and put a dummy config). With this skeleton in place, they can gradually fill in actual server checks and real SMS sending, knowing the overall wiring is solid.

## Step 7: Fill in What You Know First  
With a scaffold running, the development now proceeds to implement the actual functionality. Step 7 advises to **start by coding the parts you are confident about first**. These are the pieces of the program that you understand well (perhaps the simpler or core logic parts identified earlier). By tackling these “low-hanging fruit” tasks, you make quick progress, validate that your scaffold structure can accommodate real code, and build momentum. This is sometimes called the “easy-first” or **obvious implementation first** strategy ([TDD – Make the test pass with Green Patterns – Paulo Clavijo](https://paucls.wordpress.com/2024/05/24/tdd-make-the-test-pass-with-green-patterns/#:~:text=Obvious%20implementation)). 

The theoretical rationale behind this approach is rooted in psychology and agile principles. Delivering a part of the functionality quickly provides early feedback and a sense of accomplishment, which can motivate and guide the subsequent work. It’s aligned with the agile idea of incremental development—each small feature completed is a potentially shippable increment. Furthermore, implementing known parts first helps in testing integration early. As each piece gets completed, you can run the program and see it doing *more* than before, verifying that everything still plays nicely together.

Kent Beck’s TDD approach even has a concept for this at the micro-scale: when writing code to make a test pass, one pattern is **“Obvious Implementation”** – if you see an immediate straightforward way to implement a passing solution, do it right away ([TDD – Make the test pass with Green Patterns – Paulo Clavijo](https://paucls.wordpress.com/2024/05/24/tdd-make-the-test-pass-with-green-patterns/#:~:text=Obvious%20implementation)). This avoids overthinking; you use the simplest correct solution you can think of. At a larger scale in our methodology, Step 7 is similar: implement the straightforward parts of the plan with confidence. For example, if one module is standard or repetitive, you can code it fully now. A known best practice is to *“implement and test each stub one at a time”* ([](https://www.cs.swarthmore.edu/~alinen/cs21/f18/w07/Week07-TDD.pdf#:~:text=Implement%20and%20test%20each%20stub,deposit%20and%20then%20implemented%20withdrawal)), starting with the ones that are easiest or most crucial.

By doing this, you get **quick wins**. Each time you fill in a stub with real code and it passes a test (or produces expected behavior in the running program), you reduce the number of unknowns and boost your confidence in the system. Additionally, some parts being done can simplify others. For instance, if you implement a data parsing function and it works, any code that depends on parsed data can now be tried with real input, possibly revealing issues early.

Another angle is risk management: sometimes “what you know” also aligns with “what is critical.” If a part of the system is both well-understood and fundamental (say, a core algorithm or data structure), implementing it first de-risks the project because you verify that core component works. That said, one might argue to do risky things first (which is Step 8’s focus, using placeholders initially). The combination of Steps 7 and 8 is essentially: do easy things for real, and use fakes for the hard things until you’re ready to tackle them. This keeps progress moving on all fronts.

It’s also worth noting the interplay with testing: as you implement known parts, you should test them (ideally with unit tests or by running the program). This ensures the part truly works as expected, and since the scaffold is running, you can integrate this part into the system and see partial results. For example, if you complete a function that calculates a metric, you can call it from main and print the result, comparing it against the expected value for your sample input. Each successfully filled piece makes the skeleton “walk” better.

From a process perspective, Step 7 often involves iteration. You pick a stub or component, implement it, run the program (maybe with placeholders still for other parts) to verify, then move to the next. It’s an application of the **“test a little, code a little”** cycle often taught in programming: add a small piece, test it in the context, then proceed. This incremental filling ensures if something goes wrong, you know which recent part likely caused it, making debugging easier.

By the end of Step 7, a significant portion of the program’s functionality is implemented (all the parts you had high certainty about). The program should be doing substantially more than the bare scaffold – maybe not everything correctly, but each completed piece adds real behavior. This stage might yield a prototype-level version of the software that, for the known scenarios, actually produces correct output or function. Remaining gaps correspond to the parts you were unsure about or particularly complex logic, which will be handled next.

#### Examples (Step 7: Fill in Known Functionality)  
- **Web Development:** Continuing the e-commerce example, suppose the developer is very familiar with database operations but less sure about payment API integration. In the scaffold, there were stubs for `reserveInventory`, `chargeCard`, and `saveOrder`. The developer decides to first implement `reserveInventory` and `saveOrder` which interact with the local database (something they know well). They write those functions fully and test them: e.g., `reserveInventory(productId, quantity)` deducts stock from the `products` table; `saveOrder(orderData)` inserts a new record into the `orders` table. They run the application (with `chargeCard` still a dummy that always “approves” the payment) to see that when they simulate an order, the inventory indeed decreases and an order record is created. These parts work on first try or after minor fixes because the developer was confident in them. Meanwhile, `chargeCard` remains a placeholder (maybe returning a fixed success response) until they address it in the next step.  
- **Game Design:** In a game scaffold, known parts might be basic movement mechanics. The developer knows how to implement character movement and collision with walls, but is unsure about the AI for enemies. So, in Step 7 they fully implement the player’s movement and physics response (gravity, jumping) and perhaps the collision detection with the environment. They test this in the running game: the player can run and jump around the level and collide with platforms correctly. Because these mechanics are straightforward for them, they get it working quickly. They also implement the scoring system for collectible items (another thing they know how to do). Enemies might still just stand in place (AI not done) or move in a simple fixed pattern. By filling in movement and scoring first, they can play a version of the game where you move and collect items to see score increment – validating a chunk of functionality while enemy AI remains a stub to tackle later.  
- **Data Analysis:** In a data processing pipeline, suppose the engineer is comfortable with data cleaning steps but less certain about a complex statistical analysis part. They implement what they know: for example, `filter_invalid(data)` and `aggregate_by_region(data)` functions (they’ve done similar grouping and cleaning in the past). After coding these, they run the pipeline on sample data and verify that invalid entries are indeed removed and the aggregation yields correct sums per region (comparing with manually computed sums from their Step 5 output). These parts were low-risk and got done easily, and now the intermediate output of the pipeline matches expectations for that stage. The part they’re less sure about, say `compute_trend_analysis(data)`, remains a stub that currently maybe returns a placeholder or blank result. But because the earlier parts are done, they can feed the cleaned and aggregated data into a stub and ensure everything up to that point works correctly.  
- **Automation:** Consider an automation script to process files and then send notifications. The developer is confident in file processing (parsing files, zipping them, etc.), but not as confident in the networking code to send notifications to a web service. So they focus first on implementing file parsing and zipping. In the scaffold, they had `parse_file(file)` and `compress_files(file_list)` as stubs. They go ahead and write these fully using known libraries (perhaps using Python’s CSV module to parse and `shutil.make_archive` to zip). They test by running the script on some sample files and see the output zip file is correctly created. The notification part (`send_notification`) remains a dummy that just logs “Notification sent.” At this stage, the script accomplishes a major part of its goal (processing files), and the developer has verified that known functionality (parsing and compressing) is correct. Only the integration with the notification service is pending, which will be handled next.  

## Step 8: Use Placeholders for the Unknown (Iteratively Refine the Hard Parts)  
The final step recognizes that some parts of the system were not well-understood or were too complex to implement initially. In Step 8, those remaining unknown or tricky parts are gradually implemented by replacing the placeholders (stubs or fake data) with real logic. The key is that you **use placeholders to keep the program working** until each unknown can be tackled—so at any point, even if some functionality is not real, the program still runs end-to-end. This allows iterative refinement: you improve one placeholder at a time into actual code, always maintaining a working program. If an unknown is especially hard, you might continue using a simplified approach or hardcoded return until you find the proper solution, a practice sometimes called “fake it till you make it” in TDD terms ([TDD – Make the test pass with Green Patterns – Paulo Clavijo](https://paucls.wordpress.com/2024/05/24/tdd-make-the-test-pass-with-green-patterns/#:~:text=Fake%20it%20,it)).

This step is where the most challenging pieces are implemented, often informed by the research or spikes done in Step 4. The use of placeholders ensures that while you work on these, the rest of the system (which you built in Steps 6-7) remains operational. For example, if a function was returning a hardcoded value as a stand-in, you now incrementally replace that with actual computation, verifying at each small increment that the system still runs and produces closer-to-correct results. Kent Beck’s TDD pattern of **“Fake it till you make it”** is very relevant: *“Initially, provide a hardcoded or simplified implementation that makes the test pass. Then, gradually replace the hardcoded parts with real implementation until the desired behavior is achieved.”* ([TDD – Make the test pass with Green Patterns – Paulo Clavijo](https://paucls.wordpress.com/2024/05/24/tdd-make-the-test-pass-with-green-patterns/#:~:text=Fake%20it%20,it)). This approach emphasizes working in **very small steps**, where after each change, the program is still in a runnable (passing tests) state. 

From a theoretical standpoint, this is an application of *incremental development* and continuous integration. By never letting the unknown parts break your build (since they always had placeholders ensuring things didn’t crash), you integrate real solutions progressively. Each unknown part can be approached using techniques best suited for it – for instance, one might use more spikes, trial and error, or even formal methods if needed. The placeholder acts as a shim that returns something plausible enough for the rest of the program. A classic example: returning a constant value or a simple formula initially, then refining it. This is a disciplined way to avoid getting stuck – rather than halting development until a hard problem is solved, you keep momentum by temporarily faking that part and coming back to it.

During Step 8, testing and comparison against the expected output (from Step 5) is crucial. As you flesh out the hard parts, you continuously run the program against the known test cases to see if the output is converging to the expected output. Initially, with placeholders, the output might have incorrect sections (e.g., “TODO” text or zero values). With each refinement, those parts should start to match the expected results. This provides a clear measure of progress. When the output fully matches the expected output for the test cases, you know the implementation is complete (at least for those scenarios).

It’s also possible that during implementing the unknowns, you realize your initial plan needs adjustment. That’s fine: the scaffolding and incremental approach makes it easier to refactor or adjust because everything else is decoupled (stubs can be changed without ripple effects). For instance, you might have to change a function’s interface once you dig into the real implementation. Because you’re working one piece at a time, you can manage those changes carefully and test immediately.

Using placeholders can also mean using **mock objects or dummy data** to simulate components that aren’t ready. For example, if an external API isn’t available yet, you might use a dummy response stored locally so that development can proceed. Then when ready, you switch the code to call the real API. This ensures that integration with external systems or not-yet-built systems doesn’t block progress. In an agile team context, one team could provide a stub of their service so another team can integrate against it until the real service is available.

By the end of Step 8, all the initially unknown or complex parts have been handled – the placeholders are gone or reduced to a minimum, and the program is “filled out” with real functionality. The software should now be producing the correct outputs and behavior for all intended cases, matching the acceptance criteria established (like the manually built output). Essentially, the skeleton you started with is now fleshed out into a full body. At this point, the methodology has guided you from a nebulous problem to a working solution in a structured, low-risk way.

#### Examples (Step 8: Replacing Placeholders with Real Implementation)  
- **Web Development:** Revisit the scenario where `chargeCard(paymentInfo)` was left as a placeholder always returning success. In Step 8, the developer now works on integrating with the actual payment gateway API. They might do this gradually: first, replace the function to make a real API call but still assume success (e.g., not handling all error responses yet). They run a test transaction and see money actually moves — the integration works, and the site still runs properly with this change. Next, they add handling for failure responses from the API, replacing the simplistic placeholder logic. They test scenarios using the gateway’s test cards for declined transactions to ensure the code now correctly handles and propagates a failure (perhaps returning an error message to the user, which previously was just a hardcoded “success”). All the while, other parts of checkout (inventory, order saving) continue to work, so they can test a full purchase flow. At completion, the once-empty `chargeCard` stub is fully implemented, and the entire checkout process – which earlier used fakes – is operational and verified against expected outcomes (e.g., an order is saved only if payment succeeded, etc.).  
- **Game Design:** The game’s enemy AI that was a stub (maybe enemies stood still or moved in a simple pattern) is now addressed. The developer incrementally codes the AI: first, perhaps making enemies move towards the player’s last known position (a basic strategy). They run the game: now enemies actively chase the player rather than idling – a big change from the placeholder behavior. If an enemy movement is too erratic, they tweak the algorithm (e.g., add a delay or a pathfinding step). They keep testing in-game until enemy behavior feels right. Also, suppose the game had a boss enemy with a very complex set of actions that was originally just a placeholder “boss does nothing.” The developer might implement one attack at a time for the boss, each time testing the fight. For example, first give the boss a basic attack; then later add a special move, ensuring the game remains balanced at each addition. By gradually replacing the boss’s no-op with real moves, they can verify each in isolation and in combination. At the end, all enemy AI placeholders are gone – every enemy behaves according to design, and the game can be played through as intended.  
- **Data Analysis:** The `compute_trend_analysis(data)` function left as a stub (or returning dummy values) earlier is now implemented using a sophisticated statistical method. The data scientist decides to implement a linear regression to identify trends. At first, they might hardcode a simple trend (e.g., “increase” if last value > first value, else “decrease”) as a temporary solution. The pipeline runs end-to-end with that naive approach – it’s not accurate for all cases, but it produces an output format that matches expected structure. Now they refine it: integrate a regression library or write the math to calculate trend line slope. They test this on the sample data and compare against the manual trends they determined in Step 2/5. If the results don’t match, they debug (maybe realize they need to log-transform the data or handle seasonality). They adjust and test again. Over a few iterations, the trend analysis function becomes fully realized, replacing the initial simplistic logic. The key is at each iteration, the pipeline still ran, and the output gradually got closer to the hand-computed truth. Eventually, the trend analysis output aligns with the expected output from the manual solution, indicating the placeholder has been successfully transformed into the real solution.  
- **Automation:** The earlier automation script had `send_notification` as a dummy. In Step 8, the engineer replaces it with code that calls an actual service (for example, an HTTP POST to a Slack webhook or an SMS API). They might first do a dry-run mode: the function formats the message and logs it, but doesn’t actually send — just to verify formatting and avoid spamming during testing. Then, they enable the real send. They run the script in a controlled test environment: perhaps using a test API key that sends an SMS to their own phone or posts to a private Slack channel. It works (they receive a message), or if not, they debug (maybe the API returned an error due to missing field; they fix that). Because the rest of the script (file processing, etc.) was already working, they can observe the full chain now: e.g., a file triggers processing and then a real notification. Initially, the content of the notification might have been a placeholder like “All good.” Now they refine it to include real data from the processing results. They test again to see that the message contains the correct summary (comparing with what they expect to see). Through iterative improvements, the notification functionality goes from a no-op to a fully functional, integrated part of the script. All placeholders are now gone, and the automation performs the entire intended job, end-to-end, with real effects.

## Conclusion and Best Practices

The eight-step methodology outlined above provides a structured framework for tackling software development in a methodical, low-risk manner. It mirrors many time-tested principles in computer science and software engineering: understand the problem deeply before jumping in (as Pólya and others advise), plan your solution and data flows (stepwise refinement and design), integrate continuously (build scaffolds and skeletons), and iterate implementation with feedback (agile and TDD practices). By immersing in the problem and designing with examples, developers ensure they are solving the right problem with the correct approach. By planning code structure and assessing knowledge gaps, they set themselves up for smooth implementation, preempting potential roadblocks through research or simplification. Building a manual final output serves as a constant reference for correctness, anchoring development to clear goals. Creating a running scaffold and filling it in piece by piece harnesses the power of incremental development — always having a working product, even if partially implemented, drastically reduces integration issues and provides continuous morale boosts as functionality comes to life.

Notably, this methodology doesn’t just apply to solo programming tasks or algorithm challenges, but scales to larger projects and team environments. For example, agile teams effectively practice steps 1–5 during sprint planning and backlog grooming by understanding user stories, defining acceptance criteria (expected outputs), and designing solutions in design discussions. Steps 6–8 happen during the sprint: developers might push a basic skeleton of a feature early and then commit multiple incremental improvements (which is evident in version control history of many projects). The emphasis on placeholders and stubs is also seen in interface-driven design, where teams agree on API contracts (stubs) and each team develops their side independently, testing against mock implementations until integration.

Each step is supported by industry literature and research: from John Sonmez and other experts stressing problem understanding ([Solving Problems, Breaking it Down | by John Sonmez | Medium](https://medium.com/@jsonmez/solving-problems-breaking-it-down-8e7b379edd97#:~:text=You%20really%20want%20to%20make,before%20attempting%20to%20solve%20it)), to Code Complete’s advocacy of pseudocode and iterative refinement ([Pseudocode or Code?](https://blog.codinghorror.com/pseudocode-or-code/#:~:text=,of%20them%20becomes%20a%20problem)), to the use of spikes in agile for learning unknowns ([What is an Agile Spike Story? How to Write, Types & Benefits](https://agilemania.com/agile-spike-story-what-is-a-spike-in-agile#:~:text=What%20is%20an%20Agile%20Spike)), and the walking skeleton approach championed by Alistair Cockburn and Martin Fowler for early end-to-end integration ([Kickstart Your Next Project with a Walking Skeleton | Code Climate](https://codeclimate.com/blog/kickstart-your-next-project-with-a-walking-skeleton#:~:text=end,1)). Empirical studies on expert programmers ([How do you tell an expert from a novice? Look at how they plan.  — WENDY HIRSCH ](https://wendyhirsch.com/blog/planning-best-practice-partii#:~:text=What%20our%20professor%20had%20offered,a%20trial%20and%20error%20approach)) confirm that investing time in planning and understanding yields better outcomes than diving in blindly. Meanwhile, the psychology of incremental progress (“small wins”) is well-known to keep teams motivated and projects on track.

In applying this methodology, a few **best practices** emerge:

- **Keep the steps iterative and fluid**: You might loop back – for instance, while implementing Step 8 you might discover a new sub-problem and go back to Step 1 for that sub-problem, re-immersing and designing a mini-solution. The steps are guidelines, not a rigid waterfall; they often overlap.
- **Maintain discipline at each step**: It’s tempting to skip to coding (Step 6) without fully doing Steps 1–5 under schedule pressure. However, the time “lost” in planning is often regained multiple times over in smoother implementation. As one professor told his students: *“Stay away from the computer until you have it mapped out on paper”*, which students learned the hard way after struggling by jumping straight to code ([How do you tell an expert from a novice? Look at how they plan.  — WENDY HIRSCH ](https://wendyhirsch.com/blog/planning-best-practice-partii#:~:text=This%20was%20the%20advice%20given,database%20mapped%20out%20on%20paper)) ([How do you tell an expert from a novice? Look at how they plan.  — WENDY HIRSCH ](https://wendyhirsch.com/blog/planning-best-practice-partii#:~:text=What%20our%20professor%20had%20offered,a%20trial%20and%20error%20approach)).
- **Use tools and documentation for support**: Modern development is eased by tools — for example, for Step 5 (manual outputs), one can use Jupyter notebooks or spreadsheets; for scaffolding, frameworks like Angular or Rails provide generators that create skeletons; for placeholders, testing libraries offer mocks and fakes. Leverage these to adhere to the methodology efficiently.
- **Continuously test against expectations**: At every stage from Step 5 onward, use the hand-defined expected outcomes as the benchmark. Automate these tests if possible. This ensures that as each piece is built, you’re moving closer to “done” and not inadvertently off-track.
- **Refactor and clean up**: After Step 8, it’s wise to revisit the code for any leftover placeholder artifacts or technical debt taken on during the iterative process (perhaps some “TODO” comments or suboptimal solutions used to get things working). Clean code and documentation will prepare the ground for future maintenance or enhancements.

By following the eight steps, developers can approach even complex projects with confidence. The methodology encourages critical thinking and continuous learning (through self-assessment and research), while also emphasizing concrete progress and results. It blends analytical rigor with practical pragmatism—much like constructing a building, you first ensure the foundation is solid (understand the problem), draw blueprints (design the solution), gather materials and knowledge (self-assessment), set up scaffolding (skeleton program), then build and refine until the structure is complete and sound. Adopting this approach can lead to more robust, correct, and maintainable software, and ultimately a more efficient and enjoyable development process.

## Sources

1. Sonmez, John. “Solving Problems, Breaking it Down.” *Medium*, 2017. – Emphasizes thoroughly understanding a problem and solving it manually with examples before coding ([Solving Problems, Breaking it Down | by John Sonmez | Medium](https://medium.com/@jsonmez/solving-problems-breaking-it-down-8e7b379edd97#:~:text=The%20most%20common%20mistake%20I,code%20as%20soon%20as%20possible)) ([Solving Problems, Breaking it Down | by John Sonmez | Medium](https://medium.com/@jsonmez/solving-problems-breaking-it-down-8e7b379edd97#:~:text=I%20am%20going%20to%20tell,the%20biggest%20secret%20in%20programming)).  
2. Hirsch, Wendy. “How do you tell an expert from a novice? Look at how they plan.” *wendyhirsch.com Blog*, 2016. – Discusses how experts spend more time analyzing problems (e.g., using paper) and outlines research on problem-solving strategies ([How do you tell an expert from a novice? Look at how they plan.  — WENDY HIRSCH ](https://wendyhirsch.com/blog/planning-best-practice-partii#:~:text=This%20was%20the%20advice%20given,database%20mapped%20out%20on%20paper)) ([How do you tell an expert from a novice? Look at how they plan.  — WENDY HIRSCH ](https://wendyhirsch.com/blog/planning-best-practice-partii#:~:text=What%20our%20professor%20had%20offered,a%20trial%20and%20error%20approach)).  
3. Pólya, George. *How to Solve It*. Princeton University Press, 1945. – Classic four-step problem-solving method: Understand the problem; Devise a plan; Carry out the plan; Review/extend ([How to Solve It - Wikipedia](https://en.wikipedia.org/wiki/How_to_Solve_It#:~:text=First%20principle%3A%20Understand%20the%20problem)) ([How to Solve It - Wikipedia](https://en.wikipedia.org/wiki/How_to_Solve_It#:~:text=Second%20principle%3A%20Devise%20a%20plan)). Reinforces Steps 1 & 2 of the methodology.  
4. Wirth, Niklaus. “Program Development by Stepwise Refinement.” *Communications of the ACM*, vol. 14, no. 4, 1971, pp. 221-227. – Introduces stepwise refinement (top-down design), i.e., decompose tasks into subtasks ([Stepwise refinement](https://www.cs.cornell.edu/courses/JavaAndDS/stepwise/stepwise.html#:~:text=The%20term%20stepwise%20refinement%20,227)) ([Stepwise refinement](https://www.cs.cornell.edu/courses/JavaAndDS/stepwise/stepwise.html#:~:text=Wirth%20said%2C%20,of%20data%20into%20data%20structures)). Basis for planning code structure (Step 3).  
5. Atwood, Jeff. “Pseudocode or Code?” *Coding Horror*, 08 May 2009. – Discusses *Code Complete*’s Pseudocode Programming Process and benefits of designing in pseudocode then coding ([Pseudocode or Code?](https://blog.codinghorror.com/pseudocode-or-code/#:~:text=One%20of%20those%20chapters%20describes,we%E2%80%99d%20first%20write%20it%20in%C2%A0pseudocode)) ([Pseudocode or Code?](https://blog.codinghorror.com/pseudocode-or-code/#:~:text=,of%20them%20becomes%20a%20problem)). Supports Step 3 (code structure planning with pseudocode and iterative refinement).  
6. Cornell University CS1110 Course Notes. “Mañana Principle and Using Placeholders in Design.” (Handout, 2017) – Explains delaying implementation by using comments and stubs, and defines top-down design ([handout-08](https://www.cs.cornell.edu/courses/cs1110/2017fa/lectures/09-14-17/handout-08.pdf#:~:text=Ma%C3%B1ana%20Principle%20%E2%80%A2%20If%20not,function%20definition%20empty%20for%20now)). Also covers function stubs that return default values ([handout-08](https://www.cs.cornell.edu/courses/cs1110/2017fa/lectures/09-14-17/handout-08.pdf#:~:text=Function%20Stubs%20Procedure%20Stubs%20%E2%80%A2,Stubs%20%E2%80%A2%20Single%20return%20statement)) ([handout-08](https://www.cs.cornell.edu/courses/cs1110/2017fa/lectures/09-14-17/handout-08.pdf#:~:text=%E2%80%A2%20Example%3A%20def%20first_four_letters,name)). Backs Steps 6–8 (scaffolding and gradual code fill-in).  
7. Swarthmore CS21 Course. “Top-Down Design Lecture Notes.” (Week07-TDD.pdf, 2018) – Provides rules for incremental development and advises writing a program skeleton with stub functions that still runs ([](https://www.cs.swarthmore.edu/~alinen/cs21/f18/w07/Week07-TDD.pdf#:~:text=Code%20stubs%20,you%20expect%20it%20to%20use)), then implementing and testing each stub one at a time ([](https://www.cs.swarthmore.edu/~alinen/cs21/f18/w07/Week07-TDD.pdf#:~:text=Implement%20and%20test%20each%20stub,deposit%20and%20then%20implemented%20withdrawal)). Aligns with Steps 6–8 process.  
8. Agile Alliance. “Spike.” *Agile Glossary*. – Defines a Spike as a time-boxed research or prototyping activity to explore unknowns in order to reduce risk and uncertainty ([What is an Agile Spike Story? How to Write, Types & Benefits](https://agilemania.com/agile-spike-story-what-is-a-spike-in-agile#:~:text=What%20is%20an%20Agile%20Spike)). Relevant to Step 4 (self-assessment and knowledge acquisition).  
9. Fowler, Martin. *Bliki: Walking Skeleton*, 2006. (Referenced in Joris Voorn’s Medium article) – Describes the Walking Skeleton practice: a minimal end-to-end implementation to validate architecture assumptions ([Walking Skeleton in Software Architecture | Medium](https://medium.com/@jorisvdaalsvoort/walking-skeletons-in-software-architecture-894168276e3f#:~:text=Martin%20Fowler%20described%20the%20walking,skeleton%20as)). Underpins Step 6 (running scaffold).  
10. Cockburn, Alistair. *“Hexagonal Architecture.”* (Referenced by Code Climate via Cockburn) – Quote: *“tiny implementation of the system that performs a small end-to-end function… links together main components… architecture and functionality can then evolve in parallel.”* ([Kickstart Your Next Project with a Walking Skeleton | Code Climate](https://codeclimate.com/blog/kickstart-your-next-project-with-a-walking-skeleton#:~:text=end,1)) This quote on Walking Skeleton supports Step 6.  
11. Rezvina, Sasha (Oren Dobzinski guest). “Kickstart Your Next Project with a Walking Skeleton.” *Code Climate Blog*, 2014. – Explains how integrating a Walking Skeleton first uncovers unknowns early and quotes Cockburn ([Kickstart Your Next Project with a Walking Skeleton | Code Climate](https://codeclimate.com/blog/kickstart-your-next-project-with-a-walking-skeleton#:~:text=Image%3A%20Walking%20Skeleton)) ([Kickstart Your Next Project with a Walking Skeleton | Code Climate](https://codeclimate.com/blog/kickstart-your-next-project-with-a-walking-skeleton#:~:text=end,1)). Reinforces the value of Steps 6 & 8 (early integration, expose uncertainty early).  
12. Beck, Kent. *Test-Driven Development: By Example*. Addison-Wesley, 2003. – Introduces TDD patterns “Fake It Till You Make It” and “Obvious Implementation”. Paraphrased in Paulo Clavijo’s blog: implement simplest thing to pass tests, use hardcoded returns then generalize ([TDD – Make the test pass with Green Patterns – Paulo Clavijo](https://paucls.wordpress.com/2024/05/24/tdd-make-the-test-pass-with-green-patterns/#:~:text=Fake%20it%20,it)) ([TDD – Make the test pass with Green Patterns – Paulo Clavijo](https://paucls.wordpress.com/2024/05/24/tdd-make-the-test-pass-with-green-patterns/#:~:text=Obvious%20implementation)). Backs Steps 7 & 8’s approach to incremental implementation.  
13. Preston-Werner, Tom. “Readme Driven Development.” *Tom’s Blog*, 23 Aug 2010. – Advocates writing documentation (e.g., README) and expected usage first: *“Write your Readme first… Until you’ve written about your software, you have no idea what you’ll be coding.”* ([Readme Driven Development](https://tom.preston-werner.com/2010/08/23/readme-driven-development#:~:text=Write%20your%20Readme%20first)). Also warns *“a perfect implementation of the wrong specification is worthless.”* ([Readme Driven Development](https://tom.preston-werner.com/2010/08/23/readme-driven-development#:~:text=I%20hear%20a%20lot%20of,something%20very%20bad%20going%20on)). Supports Steps 1 & 5 (begin with the end in mind, define output/usage early).  
14. Adzic, Gojko. *Specification by Example*. Manning, 2011. – Promotes collaborative definition of requirements through concrete examples (tests) before implementation. While not directly cited above, it conceptually supports Step 5 (build expected outputs as acceptance criteria) and Step 2.  
15. Jane Street Tech Blog. “What if writing tests was a joyful experience? – Expect Tests.” 2020. – Discusses writing expected outputs by hand (in tests) vs. having the framework capture them. Emphasizes defining expected behavior up front in tests (especially in TDD) ([Jane Street Tech Blog - What if writing tests was a joyful experience? ](https://blog.janestreet.com/the-joy-of-expect-tests/#:~:text=I%20hear%20you%20already%3A%20tests,to%20write%20tests%20for%20it)). Lends support to Step 5 (manual output as test oracle).  

